{
  
    
        "post0": {
            "title": "Transfer Learning",
            "content": "Introduction . Transfer learning allows us to use trained layers from other models and applying them to our models. We can save time and resources by not having to train large networks. . Objective . We want to classify the images in the CIFAR10 dataset. It contains 60,000 images in 10 classes. . Approach . Here we use a well known pre-trained model (ResNet50) for image classification. On top of it, we add some fully connected layers to classify the learned features into ten categories. . 1.Import Libraries . import math import tensorflow as tf import matplotlib.pyplot as plt import numpy as np from tensorflow.keras import Model from tensorflow.keras import layers from tensorflow.keras.applications import ResNet50 . 2. Load the dataset . CIFAR10 dataset is readily accessible through keras. . (train_img, train_lbl), (test_img, test_lbl) = tf.keras.datasets.cifar10.load_data() . Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz 170500096/170498071 [==============================] - 12s 0us/step 170508288/170498071 [==============================] - 12s 0us/step . 3. Preprocess images . Pre-processing converts RGB to BGR, zero-centers each color channel with respect to the ImageNet dataset. . train_img = (train_img).astype(&#39;float32&#39;) test_img = (test_img).astype(&#39;float32&#39;) train_img = tf.keras.applications.resnet50.preprocess_input(train_img) test_img = tf.keras.applications.resnet50.preprocess_input(test_img) NUM_EPOCHS = 20 print(&#39;Test images: &#39;, test_img.shape) print(&#39;Train images: &#39;, train_img.shape) . Test images: (10000, 32, 32, 3) Train images: (50000, 32, 32, 3) . 4. Load ResNet50 model . Use weights from imagenet, do not include the top layers, and freeze each layer to not-trainable. We want to use the trained model as is with some modifications. . pre_trained_model = ResNet50(input_shape = (224, 224, 3), weights=&#39;imagenet&#39;, classes = 10, include_top = False) for layer in pre_trained_model.layers: layer.trainable = False . Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5 94773248/94765736 [==============================] - 0s 0us/step 94781440/94765736 [==============================] - 0s 0us/step . 5. Create model . Specify shape of images for input. | Upsample image size by 7x to match ResNet50 architecture. | Add the pre-trained model. | Flatten the output of the model. | Add a few dense layers to adapt the model to your dataset. Use the relu activation function. | Specify the output with 10 output categories and a softmax activation. | . inputs = tf.keras.layers.Input(shape=(32,32,3)) x = tf.keras.layers.UpSampling2D(size=(7,7))(inputs) x = (pre_trained_model)(x) x = layers.GlobalAveragePooling2D()(x) x = layers.Dense(1024, activation=&#39;relu&#39;)(x) x = layers.Dropout(0.2)(x) x = layers.Dense(512, activation=&#39;relu&#39;)(x) x = layers.Dense(10, activation=&#39;softmax&#39;)(x) tx_model = tf.keras.Model(inputs = inputs, outputs = x) . 6. Compile model . Use Stochastic Gradient Descent for optimization, sparse categorical crossentropy as the loss parameter, and accuracy as the metric. Then fit the model using the training images and labels. . tx_model.compile(optimizer=&#39;SGD&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics = [&#39;accuracy&#39;]) tx_model.summary() history = tx_model.fit(train_img, train_lbl, batch_size=256, epochs = NUM_EPOCHS, validation_data = (test_img, test_lbl), verbose = 1) . Model: &#34;model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 32, 32, 3)] 0 up_sampling2d (UpSampling2D (None, 224, 224, 3) 0 ) resnet50 (Functional) (None, 7, 7, 2048) 23587712 global_average_pooling2d (G (None, 2048) 0 lobalAveragePooling2D) dense (Dense) (None, 1024) 2098176 dropout (Dropout) (None, 1024) 0 dense_1 (Dense) (None, 512) 524800 dense_2 (Dense) (None, 10) 5130 ================================================================= Total params: 26,215,818 Trainable params: 2,628,106 Non-trainable params: 23,587,712 _________________________________________________________________ Epoch 1/20 196/196 [==============================] - 90s 408ms/step - loss: 1.1992 - accuracy: 0.5885 - val_loss: 0.8620 - val_accuracy: 0.7040 Epoch 2/20 196/196 [==============================] - 79s 401ms/step - loss: 0.8300 - accuracy: 0.7135 - val_loss: 0.7340 - val_accuracy: 0.7436 Epoch 3/20 196/196 [==============================] - 79s 401ms/step - loss: 0.7389 - accuracy: 0.7425 - val_loss: 0.6749 - val_accuracy: 0.7644 Epoch 4/20 196/196 [==============================] - 79s 401ms/step - loss: 0.6850 - accuracy: 0.7631 - val_loss: 0.6593 - val_accuracy: 0.7700 Epoch 5/20 196/196 [==============================] - 79s 401ms/step - loss: 0.6479 - accuracy: 0.7757 - val_loss: 0.6292 - val_accuracy: 0.7794 Epoch 6/20 196/196 [==============================] - 79s 401ms/step - loss: 0.6174 - accuracy: 0.7851 - val_loss: 0.6192 - val_accuracy: 0.7809 Epoch 7/20 196/196 [==============================] - 79s 401ms/step - loss: 0.5981 - accuracy: 0.7910 - val_loss: 0.6085 - val_accuracy: 0.7889 Epoch 8/20 196/196 [==============================] - 78s 401ms/step - loss: 0.5761 - accuracy: 0.8007 - val_loss: 0.5799 - val_accuracy: 0.7978 Epoch 9/20 196/196 [==============================] - 79s 401ms/step - loss: 0.5592 - accuracy: 0.8061 - val_loss: 0.5734 - val_accuracy: 0.7996 Epoch 10/20 196/196 [==============================] - 78s 401ms/step - loss: 0.5449 - accuracy: 0.8113 - val_loss: 0.5621 - val_accuracy: 0.8046 Epoch 11/20 196/196 [==============================] - 78s 401ms/step - loss: 0.5319 - accuracy: 0.8142 - val_loss: 0.5693 - val_accuracy: 0.8010 Epoch 12/20 196/196 [==============================] - 79s 401ms/step - loss: 0.5186 - accuracy: 0.8193 - val_loss: 0.5430 - val_accuracy: 0.8121 Epoch 13/20 196/196 [==============================] - 78s 401ms/step - loss: 0.5085 - accuracy: 0.8208 - val_loss: 0.5291 - val_accuracy: 0.8145 Epoch 14/20 196/196 [==============================] - 78s 401ms/step - loss: 0.4980 - accuracy: 0.8261 - val_loss: 0.5256 - val_accuracy: 0.8197 Epoch 15/20 196/196 [==============================] - 79s 401ms/step - loss: 0.4877 - accuracy: 0.8309 - val_loss: 0.5555 - val_accuracy: 0.8056 Epoch 16/20 196/196 [==============================] - 79s 401ms/step - loss: 0.4759 - accuracy: 0.8342 - val_loss: 0.5302 - val_accuracy: 0.8123 Epoch 17/20 196/196 [==============================] - 78s 401ms/step - loss: 0.4670 - accuracy: 0.8378 - val_loss: 0.5361 - val_accuracy: 0.8112 Epoch 18/20 196/196 [==============================] - 78s 401ms/step - loss: 0.4585 - accuracy: 0.8411 - val_loss: 0.5069 - val_accuracy: 0.8228 Epoch 19/20 196/196 [==============================] - 78s 401ms/step - loss: 0.4513 - accuracy: 0.8433 - val_loss: 0.5061 - val_accuracy: 0.8254 Epoch 20/20 196/196 [==============================] - 79s 401ms/step - loss: 0.4446 - accuracy: 0.8441 - val_loss: 0.5036 - val_accuracy: 0.8217 . 7. Plot results . Plot the response (accuracy and loss) by epochs for the training and validation sets. . acc=history.history[&#39;accuracy&#39;] val_acc=history.history[&#39;val_accuracy&#39;] loss=history.history[&#39;loss&#39;] val_loss=history.history[&#39;val_loss&#39;] epochs=range(len(acc)) plt.plot(epochs, acc, &#39;o-r&#39;, label=&#39;Training&#39;) plt.plot(epochs, val_acc, &#39;o--b&#39;, label=&#39;Test&#39;) plt.title(&#39;Accuracy: Training and Validation&#39;) plt.legend(loc=&quot;lower right&quot;) plt.figure() plt.plot(epochs, loss, &#39;o-r&#39;, label=&#39;Training&#39;) plt.plot(epochs, val_loss, &#39;o--b&#39;, label=&#39;Test&#39;) plt.title(&#39;Loss: Training and Validation&#39;) plt.legend(loc=&quot;upper right&quot;) . &lt;matplotlib.legend.Legend at 0x7fb97e577ad0&gt; .",
            "url": "https://joshisaumil.github.io/ml-blog/jupyter/2022/02/07/transfer-learning.html",
            "relUrl": "/jupyter/2022/02/07/transfer-learning.html",
            "date": " • Feb 7, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Getting started with Jupyter",
            "content": "Jupyter notebook . Jupyter Notebook is a web application for creating and sharing computational (code + notebooks + data) documents. JupyterLab is the next generation implementation of Jupyter notebooks.1 . Jupyter architecture consists of . a kernel (which executes code) | a client (the browser) | a notebook (contains code, metadata, contents, outputs) | a notebook server (saves and loads notebooks) | . Documentation: Jupyter . Setup . Create a virtual environment . python3 -m venv py3 . Activate a virtual environment . source py3/bin/activate . Install Jupyter Lab . pip install jupyterlab pip install ipykernel . Add kernel . ipython kernel install --user --name=py3 . Launch Jupyter Lab . jupyter-lab . To list installed kernels . jupyter kernelspec list . To uninstall kernels . jupyter kernelspec uninstall py3 . Footnotes . Project jupyter [Internet]. [cited 2022 Jan 24]. Available from: https://jupyter.org &#8617; . |",
            "url": "https://joshisaumil.github.io/ml-blog/markdown/2022/01/22/jupyter.html",
            "relUrl": "/markdown/2022/01/22/jupyter.html",
            "date": " • Jan 22, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Scheduling Cron Jobs with Crontab",
            "content": "Introduction . Setting up scheduled jobs in unix/mac is simple. . Here is a description of cron. . There are five steps to create a simple task. . Describe the task . Set up a bash script that describes the task. . In the terminal, change directory to a local folder and edit a new file. . cd ~/path/to/dir nano hello.sh . Add the following lines to the file, press control + X to save and exit. . now=$(date) echo &quot;Hello World, it&#39;s $now&quot; . Schedule the task . In the terminal, change directory to a local folder and edit a new file. . cd ~/path/to/dir nano cron . Add the following line to the file, press control + X to save and exit. . 31 16 * * * cd ~/path/to/dir/ &amp;&amp; bash ./hello.sh &gt;| hello.log . The cron task syntax is as follows: . (minute) (hour) (day of the month) (month) (day of the week) &lt;command&gt; . Specifying a * in place of each parameter defaults to all. The example above refers to a task that runs every day of every month at 4:31 pm local time. . Submit the task . In the terminal, type the following to submit the tasks in the cronjob file. . crontab cronjob . Verify that crontab was updated with the new task list. In the terminal, type the following to list the tasks. . crontab -l . Check results . Check the output result after the task has run. . In the terminal, type the following to see the output. . cat hello.log . Here is a sample output . Hello World, it&#39;s Wed Mar 31 16:31:00 MDT 2021 . There is a lot you can do with crontab. The above will hopefully help you get started. . Other useful resources: https://ole.michelsen.dk/blog/schedule-jobs-with-crontab-on-mac-osx https://opensource.com/article/17/11/how-use-cron-linux https://crontab.guru/ .",
            "url": "https://joshisaumil.github.io/ml-blog/markdown/2022/01/17/cron.html",
            "relUrl": "/markdown/2022/01/17/cron.html",
            "date": " • Jan 17, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Welcome",
            "content": "",
            "url": "https://joshisaumil.github.io/ml-blog/markdown/2022/01/15/welcome.html",
            "relUrl": "/markdown/2022/01/15/welcome.html",
            "date": " • Jan 15, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "About . Linkedin Google Scholar .",
          "url": "https://joshisaumil.github.io/ml-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://joshisaumil.github.io/ml-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}